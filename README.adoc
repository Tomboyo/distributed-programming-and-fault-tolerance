= Fault Tolerance Exercises

This repository consists of exercises that work with concepts in the field of distributed fault tolerance. Each exercise explains a specific technology of interest and introduces the reader to a demonstration application that utilizes or implements the technology.

== Circuit Breaker

In this exercise we use a _circuit breaker_ to protect an HTTP call to an underlying service. Circuit breakers are a tool to shed load from struggling subsystems, decrease latency of requests by failing fast, introduce subsystem monitoring, and give operations teams granular control over system interactions <<Fowler>><<Netflix>>.

A circuit breaker is essentially a protective sleeve around a function call. Clients make requests to the breaker instead of to the protected function, and then the breaker selectively chooses to forward those requests on to the protected function. If the function becomes unresponsive or erroneous, the breaker may "trip" and refuse to forward subsequent requests.

In this way a breaker can reduce load on strained subsystems accessed by its protected call. This not only gives the strained subsystem an opportunity to recover <<Fowler>><<Netflix>>, it also prevents the client from potentially dedicating resources to a slow subsystem <<Fowler>><<Netflix>>, allowing those resources to be used for productive work. The client of a tripped breaker may furthermore fall back on secondary measures (e.g cached data or default values); while not ideal, this ensures end-users receive usable data within an acceptable window of time <<Netflix>>.

A circuit breaker may generate logs, alerts, or other diagnostics when it is tripped, informing operators of possible system degradation, and may even expose an interface to operators which allows them to manually toggle the breaker between states for troubleshooting <<Fowler>>.

The following sections demonstrate these capabilities of the circuit breaker.

=== Design

This exercise consists of a spring application utilizing https://github.com/resilience4j/resilience4j[resilience4j] spring annotations. The application listens and responds to HTTP requests after a configurable delay via `FaultyController` and additionally issues such requests to itself once every second via the `Requester`. The `Requester` makes calls to the service by proxy of a circuit breaker, which may prevent the `Requester` from actually issuing an HTTP request depending on the breaker's internal state.

The breaker is configured to trip into the `OPEN` state and deny requests after a single slow or erroneous response from the protected HTTP call. It will remain `OPEN` for a while and deny subsequent requests, then transition to `HALF-OPEN` and allow a single probe request through. If the probe is still unhealthy, the breaker returns to the `OPEN` state and continues to deny requests. Otherwise it will `CLOSE` and allow all requests through until another slow response trips the breaker. The breaker configuration is defined in the `client/src/main/resources/application.yaml` file by the `resilience4j.circuitbreaker` element.

The `FaultyController` responds to requests after a delay; this deplay may be configured by sending a request to `POST localhost:8080/`, for example:

[source, bash]
----
curl localhost:8080/ -H 'Content-Type: application/json' -d '{"delay_millis": 2000}'
----

Reconfiguring the delay lets us create conditions under which the breaker my trip. For simplicity's sake our server and client run in the same application, but were that not so we could also simply crash the server to trip the breaker running on the client.

The resilience4j team is, as of 2020-July-11, https://github.com/resilience4j/resilience4j/pull/1038[working on Spring Boot Actuator support] to expose circuit breaker state transitions to operators via a REST API. This has not yet been released, but will allow operators to manually change the state of a circuit breaker to one of the `FORCED_OPEN`, `DISABLED`, or `CLOSED` states. While `FORCED_OPEN`, a breaker will deny all traffic. While `DISABLED`, the breaker will instead allow all traffic. In either case the breaker will not change state on its own and must be forced `CLOSED` to resume normal operation. Until the actuator endpoint is released, though, we have replicated this behavior with a simple controller endpoint (`CircuitBreakerController`). It defines a '/circuitbreakers/faultyservice-ping' POST handler that will allow the user to force the state with a call like the following:

[source, bash]
----
curl localhost:8081/circuitbreakers/faultyservice-ping -H 'Content-Type: application/json' -d '{"updateState": "CLOSED"}'
----

==== Resilience4J

We should take a moment to note that the Resilience4J implements a number of useful fault-tolerance mechanisms like circuit breakers, bulkheads, and retry mechanisms. Resilience4j is inspired by Netflix's https://github.com/Netflix/Hystrix[Hystrix] library, which is no longer actively maintained but curates similar configuration-based mechanisms. (As stated in the Hystrix README, Netflix is moving away from configuration-heavy strategies employed by Resilience4J and Hystix towards research on https://medium.com/@NetflixTechBlog/performance-under-load-3e6fa9a60581[adaptive concurrency limits].)

=== Demos

To run the circuitbreaker demo application, invoke the following from the root directory of the project:

`./gradlew bootRun --args='--mode=circuitbreaker'`

Once running, work through any of the following demos in this section.

==== Circuitbreaker state changes

When the application starts, the `Requester`'s circuit breaker is initially in the `CLOSED` state and will therefore issue `GET localhost:8080/` requests to `FaultyController`. The controller responds after a 200 ms delay by default, which the Client considers to be a healthy response time. As such, the following healthy logs are initially printed:

----
...
21:19:50.218 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 222 ms
21:19:51.214 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 217 ms
21:19:52.213 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 217 ms
----


If we make the following POST request, the Faulty Service will not respond to requests until after 700 ms, which is slower than the Client considers to be healthy:

----
curl localhost:8080/ -H 'Content-Type: application/json' -d '{"delay_millis": 700}'
----

As a result, the Client's circuit breaker will trip into the `open` state and deny subsequent requests to the protected HTTP GET method. At this point, Faulty Service is no longer receiving traffic.

----
...
21:20:24.719 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Breaker faultyservice-ping transition: CLOSED -> OPEN
21:20:24.720 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 724 ms
21:20:25.000 ERROR [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call failed: CircuitBreaker 'faultyservice-ping' is OPEN and does not permit further calls
21:20:25.997 ERROR [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call failed: CircuitBreaker 'faultyservice-ping' is OPEN and does not permit further calls
----

NOTE: The Requester logs round-trip time (`Call OK in ...`) after the request to the breaker is evaluated, so transition events (`transition: CLOSED -> OPEN`) are logged before the elapsed time.

After an interval, however, the breaker will transition to the `HALF-OPEN` state. It will allow one request through to the protected HTTP method in order to assess the health of the underlying service. If Faulty Service is left as we have configured it, the probe request will take a little over 700 ms and again trip the breaker into the `OPEN` state.

----
...
21:20:28.997 ERROR [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call failed: CircuitBreaker 'faultyservice-ping' is OPEN and does not permit further calls
21:20:29.998 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Breaker faultyservice-ping transition: OPEN -> HALF_OPEN
21:20:30.709 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Breaker faultyservice-ping transition: HALF_OPEN -> OPEN
21:20:30.710 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 714 ms
21:20:30.997 ERROR [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call failed: CircuitBreaker 'faultyservice-ping' is OPEN and does not permit further calls
21:20:31.997 ERROR [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call failed: CircuitBreaker 'faultyservice-ping' is OPEN and does not permit further calls
----

However, if we reconfigure Faulty Service to respond after a short 100 ms delay,

[source, bash]
----
curl localhost:8080/ -H 'Content-Type: application/json' -d '{"delay_millis": 100}'
----

then when the breaker next enters the `HALF-OPEN` state, the probe request will resolve quickly and the breaker will transition into the `CLOSED` state. In this state, all requests to the breaker's protected HTTP method are made. Faulty Service now receives 100% of attempted traffic.

----
...
21:23:28.996 ERROR [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call failed: CircuitBreaker 'faultyservice-ping' is OPEN and does not permit further calls
21:23:29.996 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Breaker faultyservice-ping transition: OPEN -> HALF_OPEN
21:23:30.108 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Breaker faultyservice-ping transition: HALF_OPEN -> CLOSED
21:23:30.109 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 113 ms
21:23:31.106 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 110 ms
21:23:32.106 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 110 ms
----

==== Circuitbreaker operations REST API

As in the previous exercise, when the application starts, the `Requester`'s circuit breaker is initially in the `CLOSED` state and will therefore issue `GET localhost:8080/` requests to `FaultyController`. The controller responds after a 200 ms delay by default, which the Client considers to be a healthy response time. As such, the following healthy logs are initially printed:

----
...
21:19:50.218 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 222 ms
21:19:51.214 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 217 ms
21:19:52.213 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 217 ms
----

If we wish to stop all traffic through the breaker nonetheless, however, we can issue a POST request to the operations endpoint and put the breaker in the `FORCED_OPEN` state:

[source, bash]
----
curl localhost:8080/circuitbreakers/faultyservice-ping -H 'Content-Type: application/json' -d '{"updateState": "FORCED_OPEN"}'
----

At this point, every request to the circuit breaker will fail with a CallNotPermitted error, and so the Requester will log errors until we change the breaker state again:

----
...
21:37:45.631 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 221 ms
21:37:46.630 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 221 ms
21:37:47.626 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 216 ms
21:37:48.453 INFO  [reactor-http-epoll-4] com.github.tomboyo.faultyservice.Requester: Breaker faultyservice-ping transition: CLOSED -> FORCED_OPEN
21:37:48.453 INFO  [reactor-http-epoll-4] com.github.tomboyo.faultyservice.CircuitBreakerController: Forced FaultyService::ping breaker state to FORCED_OPEN
21:37:48.618 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 209 ms
21:37:49.413 ERROR [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call failed: CircuitBreaker 'faultyservice-ping' is FORCED_OPEN and does not permit further calls
21:37:50.410 ERROR [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call failed: CircuitBreaker 'faultyservice-ping' is FORCED_OPEN and does not permit further calls
21:37:51.410 ERROR [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call failed: CircuitBreaker 'faultyservice-ping' is FORCED_OPEN and does not permit further calls
----

We could next force the breaker into the `DISABLED` state:

[source, bash]
----
curl localhost:8080/circuitbreakers/faultyservice-ping -H 'Content-Type: application/json' -d '{"updateState": "DISABLED"}'
----

This will allow requests through no matter what, and so the requester should go back to logging `Call OK` lines.

----
...
21:39:52.410 ERROR [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call failed: CircuitBreaker 'faultyservice-ping' is FORCED_OPEN and does not permit further calls
21:39:52.595 INFO  [reactor-http-epoll-5] com.github.tomboyo.faultyservice.Requester: Breaker faultyservice-ping transition: FORCED_OPEN -> DISABLED
21:39:52.595 INFO  [reactor-http-epoll-5] com.github.tomboyo.faultyservice.CircuitBreakerController: Forced FaultyService::ping breaker state to DISABLED
21:39:53.620 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 211 ms
21:39:54.624 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 215 ms
----

If we then instruct the Faulty Service to take a long time to respond,

[source, bash]
----
curl localhost:8080/ -H 'Content-Type: application/json' -d '{"delay_millis": 700}'
----

the breaker will not trip and will allow requests through regardless:

----
...
21:41:39.615 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 206 ms
21:41:40.618 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 208 ms
21:41:41.618 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 208 ms
21:41:43.117 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 708 ms
21:41:44.117 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 708 ms
21:41:45.118 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 708 ms
----

When we are done manually overriding the breaker behavior, we can set it back to `CLOSED`:

[source, bash]
----
curl localhost:8080/circuitbreakers/faultyservice-ping -H 'Content-Type: application/json' -d '{"updateState": "CLOSED"}'
----

The breaker will resume normal operation at this point.

----
...
21:42:40.121 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 712 ms
21:42:41.116 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 707 ms
21:42:42.117 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 707 ms
21:42:42.223 INFO  [reactor-http-epoll-7] com.github.tomboyo.faultyservice.Requester: Breaker faultyservice-ping transition: DISABLED -> CLOSED
21:42:42.223 INFO  [reactor-http-epoll-7] com.github.tomboyo.faultyservice.CircuitBreakerController: Forced FaultyService::ping breaker state to CLOSED
21:42:43.120 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Breaker faultyservice-ping transition: CLOSED -> OPEN
21:42:43.120 INFO  [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call OK in 711 ms
21:42:43.410 ERROR [scheduling-1] com.github.tomboyo.faultyservice.Requester: Call failed: CircuitBreaker 'faultyservice-ping' is OPEN and does not permit further calls
----

== Rate Limiter

In this exercise we use a _rate limiter_ to limit the rate of requests made against a system.

Rate-limiters exist to protect services from unsustainable request volumes, such as during spikes in user traffic or denail-of-service attacks (including friendly-fire from misbehaving collaborator services) <<google-cloud>><<stripe>>. While services can utilize rate-limiting to protect themselves from high request volume, they can also use rate-limiting to control the volume of their own requests to unprotected legacy back-ends that are otherwise vulnerable <<google-cloud>>. In either case, rate-limiters deny requests above a certain throughput, shedding load off vulnerable systems, and may even prioritize certain types of requests over others to ensure less-significant systems degrade before more-critical systems <<stripe>>.

Clients of rate-limited systems should respond to rate-limiting intelligently or else risk https://landing.google.com/sre/sre-book/chapters/addressing-cascading-failures/[cascading failures] as errors from one service propagate uncontrolled throughout a network of collaborting services, possibly with self-reinforcing consequences <<google-cloud>>. In order for clients to respond to rate-limiting, services must signal clients when rate-limiting is active, such as by returning error code https://httpstatuses.com/429[429: Too Many Requests]<<getambassador>> during periods of high load. Clients will typically want to re-try rate-limited requests, though they should do so with exponential back-off and jitter to avoid the https://en.wikipedia.org/wiki/Thundering_herd_problem["thundering herd"] problem wherein many clients inadvetently harmonize their requests, resulting in large spikes of requests to the service <<google-cloud>>. As an alternative to rejecting excessive requests, services may be able to enqueue them for asynchronous fulfillment. In this scenario a service would immediatley return a job ID to the client in response to their request. The client would know to poll the service or subscribe to an appropriate messaging channel for the response <<resilience4j-ratelimiter>><<google-cloud>>.

Public-facing services may need to make special considerations in how they respond to users when rate-limiting requests. In Mahdi's case <<figma>>, malicious users created new spam accounts in reponse to 429 signals from Figma's services, allowing the attackers to circumvent the per-user rate-limiting strategy. Figma updated their rate-limiting tactic to return 200 codes instead of the usual 429 so that rate-limiting was not apparent to the end-user, thereby placing a https://en.wikipedia.org/wiki/Shadow_banning[stealth-ban] the malicious users such that they no longer knew when to create new accounts. An alternative strategy may be to restrict new accounts to a limited subset of services so they cannot be easily used to create spam content.

There are multiple algorithms for rate-limiting <<google-cloud>><<wikipedia-rate-limiting>><<figma>>, such as the token bucket, leaky bucket, sliding window, and fixed window algorithms.

[bibliography]
== References
// circuit breaker
- [[[Fowler, 1]]] M. Fowler. "CircuitBreaker." martinFowler.com. https://www.martinfowler.com/bliki/CircuitBreaker.html (accessed June 29, 2020).
- [[[Netflix, 2]]] B. Christensen. "Fault tolerance in a high volume, distributed system." The Netflix Tech Blog. https://netflixtechblog.com/fault-tolerance-in-a-high-volume-distributed-system-91ab4faae74a (accessed June 29, 2020).
// rate-limiter
- [[[google-cloud, 3]]] "Rate-limiting strategies and techniques." Google Cloud. https://cloud.google.com/solutions/rate-limiting-strategies-techniques (accessed Aug. 8, 2020).
- [[[stripe, 4]]] P. Tarjan. "Scaling your API with rate limiters." Stripe. https://stripe.com/blog/rate-limiters (accessed Aug. 8, 2020).
- [[[getambassador, 5]]] D. Bryant. "Part 2: rate limiting for API gateways." Ambassador. https://blog.getambassador.io/rate-limiting-for-api-gateways-892310a2da02 (accessed Aug. 8, 2020).
- [[[resilience4j-ratelimiter, 6]]] "Ratelimiter." Resilience4j. https://resilience4j.readme.io/docs/ratelimiter (accessed Aug. 8, 2020).
- [[[figma, 7]]] N. Mahdi. "An alternative approach to rate limiting." Medium. https://medium.com/figma-design/an-alternative-approach-to-rate-limiting-f8a06cf7c94c (accessed Aug. 8, 2020).
- [[[wikipedia-ratelimiting, 8]]] "Rate limiting." Wikipedia. https://en.wikipedia.org/wiki/Rate_limiting (accessed Aug. 8, 2020).
